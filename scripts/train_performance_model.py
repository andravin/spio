import argparse

import pandas as pd
import xgboost as xgb

try:
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import mean_squared_error
except ImportError as e:
    raise ImportError(
        "scikit-learn is required to train the performance model. "
        "Please install it using `pip install scikit-learn`."
    ) from e

from spio.kernels import (
    Conv2dGw8Params,
    Conv2dGw8Config,
    Conv2dGw8WgradConfig,
    get_device_performance_model_file_name,
    PERFORMANCE_MODEL_EXTENSION,
)
from spio.util import import_dataclass_column

PARAMS_CLASSES = dict(Conv2dGw8Params=Conv2dGw8Params)
CONFIG_CLASSES = dict(
    Conv2dGw8Config=Conv2dGw8Config, Conv2dGw8WgradConfig=Conv2dGw8WgradConfig
)

# Suppress SettingWithCopyWarning
pd.options.mode.chained_assignment = None


def main():
    parser = argparse.ArgumentParser(
        description="Train a performance model for a given kernel."
    )
    parser.add_argument(
        "--datafile",
        required=True,
        type=str,
        default=None,
        help="Path to SSV file generated by spio/benchmark.py",
    )
    parser.add_argument(
        "--device",
        type=str,
        default=None,
        help="The name of the device to build the model for",
    )
    parser.add_argument(
        "--arch", type=str, default=None, help="The compute capability of the device"
    )
    parser.add_argument(
        "--kernel",
        type=str,
        default=None,
        help="The name of the kernel to build the model for",
    )
    parser.add_argument(
        "--outputfile",
        type=str,
        default=None,
        help="Optional file to save the model to. Otherwise the filename is generated automatically.",
    )
    args = parser.parse_args()

    first_record = get_first_record(args.datafile)
    default_device = first_record["Device"]
    default_arch = first_record["Arch"]
    default_kernel = first_record["Kernel"]
    device = args.device if args.device is not None else default_device
    arch = args.arch if args.arch is not None else default_arch
    kernel = args.kernel if args.kernel is not None else default_kernel

    df = pd.read_csv(args.datafile, delimiter=";")

    # Filter the data for the specified device, architecture, and kernel.
    df = df[(df["Device"] == device) & (df["Arch"] == arch) & (df["Kernel"] == kernel)]

    print(f"Training model for {args} on {device} ({arch})")
    print(f"Number of samples: {len(df)}")

    # Drop unused features
    df.drop(
        columns=[
            "Kernel",
            "Kernel_Kwargs",
            "Device",
            "Arch",
            "TFLOP/s",
            "Eff BW[GB/s]",
            "Idx",
        ],
        inplace=True,
    )

    # Separate the features and target.
    X = df[["Params", "Config"]]
    y = df["Time[ms]"]

    X = import_dataclass_column(X, "Params", PARAMS_CLASSES)
    X = import_dataclass_column(X, "Config", CONFIG_CLASSES)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # Convert data to DMatrix format (optimized for XGBoost)
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dtest = xgb.DMatrix(X_test, label=y_test)

    # Set XGBoost parameters
    params = {
        "objective": "reg:squarederror",  # Regression task
        "max_depth": 6,  # Maximum depth of the trees
        "eta": 0.1,  # Learning rate
        "subsample": 0.8,  # Fraction of data to be used for each tree
        "colsample_bytree": 0.8,  # Fraction of features to be used for each tree
        "seed": 42,  # For reproducibility
    }

    # Train the model
    num_rounds = 100  # Number of boosting rounds
    bst = xgb.train(params, dtrain, num_rounds)

    # Evaluate the model on the test set
    y_pred = bst.predict(dtest)
    mse = mean_squared_error(y_test, y_pred)
    print(f"Mean Squared Error: {mse:.3f} milliseconds")

    if args.outputfile is None:
        args.outputfile = get_device_performance_model_file_name(kernel, device, arch)

    if "." not in args.outputfile:
        args.outputfile += PERFORMANCE_MODEL_EXTENSION

    print(f"Saving model to {args.outputfile}")

    bst.save_model(args.outputfile)


def get_first_record(datafile):
    field_names = []
    with open(datafile, "r") as f:
        for line in f:
            line = line.strip()
            if line:
                tokens = line.split(";")
                # Skip header
                if tokens[0] == "Kernel":
                    field_names = tokens
                    continue
                record = dict()
                for field_name, token in zip(field_names, tokens):
                    record[field_name] = token
                return record
    return None


if __name__ == "__main__":
    main()
