import argparse
import sys

import torch
import ast

import spio
import spio.kernels
import spio.reflection


# Same as the pytorch-image-models function, but auto-generated by github copilot!
class ParseKwargs(argparse.Action):
    def __call__(self, parser, namespace, values, option_string=None):
        kw = {}
        for value in values:
            key, value = value.split("=")
            try:
                kw[key] = ast.literal_eval(value)
            except ValueError:
                kw[key] = str(
                    value
                )  # fallback to string (avoid need to escape on command line)
        setattr(namespace, self.dest, kw)


parser = argparse.ArgumentParser()
parser.add_argument("--kernel-class", type=str, default="Conv2dGw8Kernel")
parser.add_argument("--params", nargs="*", required=True, action=ParseKwargs)
parser.add_argument("--config", nargs="*", default={}, action=ParseKwargs)
parser.add_argument("--kernel-kwargs", nargs="*", default={}, action=ParseKwargs)
parser.add_argument("--auto-tune", action="store_true")
parser.add_argument("--warmup", type=int, default=100)
parser.add_argument("--num-iters", type=int, default=100)
parser.add_argument("--depth", type=int, default=1)
parser.add_argument("--benchmark-torch", action="store_true")
parser.add_argument("--skip-kernels", action="store_true")
args = parser.parse_args()

# This is essential for torch kernel performance:
torch.backends.cudnn.benchmark = True

kernel_class = getattr(spio.kernels, args.kernel_class)
params = kernel_class.Params(**args.params)
reflection = spio.reflection.get_kernel_reflection(kernel_class)

if not args.skip_kernels:
    if len(args.config) == 0:
        configs = None
    else:
        configs = [kernel_class.Config(**args.config)]

    if reflection.stackable:
        args_lst = reflection.make_stacked_args(params, depth=args.depth)
    else:
        args_lst = [reflection.make_args(params) for _ in range(args.depth)]
    kernel_args_lst = [reflection.arrange_kernel_args(args) for args in args_lst]
    spio.kernels.benchmark(
        kernel_class,
        params,
        kernel_args_lst,
        configs=configs,
        warmup=args.warmup,
        num_iters=args.num_iters,
        **args.kernel_kwargs,
    )

if args.benchmark_torch:
    if reflection.reference is None:
        print(f"No torch function defined for {args.kernel_class}.")
        sys.exit(1)
    if not args.skip_kernels:
        print()
    if reflection.is_grad:
        spio.kernels.benchmark_grad_reference(
            reflection,
            kernel_class,
            params,
            warmup=args.warmup,
            num_iters=args.num_iters,
        )
    else:
        torch_func = reflection.reference
        torch_args_lst = reflection.arrange_stacked_args_for_function(args_lst)
        spio.kernels.benchmark_function(
            kernel_class,
            reflection.reference,
            params,
            torch_args_lst,
            warmup=args.warmup,
            num_iters=args.num_iters,
        )
